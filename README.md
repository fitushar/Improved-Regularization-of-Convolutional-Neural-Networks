# Improved-Regularization-of-Convolutional-Neural-Networks

In order to combat overfitting during the training process, researchers have introduced regularization techniques. Regularization techniques incorporate changes into the model in order to reduce the generalization error without harming the training error. A performance improvement can be expected if the regularization technique is chosen carefully. Commonly regularization techniques include dataset augmentation, layer dropout, and weight penalty regularization (L1- or L2-based). Each technique uses a different approach to increase generalizability of the model. Recently, several novel regularization techniques have been introduced to further combat the overfitting issue in large-sized models. In this study, we aim to evaluate the effectiveness of three novel augmentation techniques, namely cutout regularization [1], mixup regularization [2], and self-supervised rotation predictor [3].





# Part of this codes were adopted from original Implementation of 

* Corrupted CIFAR10: https://github.com/tanimutomo/cifar10-c-eval 
* Mixup: https://github.com/facebookresearch/mixup-cifar10
* Auxialiary rotation loss: https://github.com/hendrycks/ss-ood


# Citation
